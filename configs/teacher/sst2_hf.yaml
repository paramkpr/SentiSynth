training:
  # ---------- bookkeeping ----------
  output_dir: "runs/teacher/deberta_v3_base"
  overwrite_output_dir: true
  run_name: "teacher_sst2_deberta_v3_base_h100"

  report_to: "wandb"
  wandb_project: "senti_synth_teacher"

  # ---------- batch size & epochs ----------
  per_device_train_batch_size: 64        # 4× bigger than before; fits easily in 80 GB
  per_device_eval_batch_size: 256        # evaluation is memory‑lighter, so push higher
  gradient_accumulation_steps: 1         # no need for micro‑batching on an H100
  num_train_epochs: 6                    # SST‑2 is small; 4 epochs normally reaches peak F1

  # ---------- precision & speed ----------
  bf16: true              # H100 has native BF16; gives ~1.8× speed‑up over FP32  
  fp16: false             # turn FP16 off to avoid two mixed‑precision modes
  # If you prefer automatic selection, drop bf16/fp16 and add `torch_dtype: "auto"`

  # ---------- optimiser & scheduler ----------
  learning_rate: 0.0001   # linear‑scale LR (16→64 batch ⇒ ×4 LR)
  warmup_ratio: 0.05      # keep warm‑up tokens roughly constant after batch change

  # ---------- misc performance knobs ----------
  dataloader_num_workers: 8      # plenty of CPU headroom; hides data‑loading latency
  gradient_checkpointing: false  # not needed; trade memory for speed
  max_grad_norm: 1.0             # good default when using larger LR + BF16

  # ---------- logging, saving, early stop ----------
  logging_steps: 100
  eval_steps: 500
  save_steps: 500
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "eval_f1"
  greater_is_better: true

  use_early_stopping: true
  early_stopping_patience: 2     # fewer epochs, so tighten patience
  early_stopping_threshold: 0.0005

  do_test_eval: true
