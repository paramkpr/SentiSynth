model:
  model_name: "gpt2"          # you can swap in "gpt2-medium" or "EleutherAI/pythia-70m" etc.
  use_fast_tokenizer: true
  block_size: 128             # Maximum sequence length after tokenisation

data:
  dataset_path: "./data/clean/" # Use HF dataset identifier
  max_len: 32
  train_split: "train"
  validation_split: "val"
  test_split: "test"

training:
  # ── bookkeeping ────────────────────────────────────────────────────────────
  output_dir: "runs/generator/gpt2_sst2"
  overwrite_output_dir: true
  run_name: "generator_sst2_gpt2"

  report_to: "wandb"
  wandb_project: "senti_synth_generator"

  # ── batch size & epochs ────────────────────────────────────────────────────
  per_device_train_batch_size: 32        # fits comfortably on 24 GB VRAM
  per_device_eval_batch_size: 64
  gradient_accumulation_steps: 1
  num_train_epochs: 3                    # SST‑2 is tiny; 2–3 epochs suffice

  # ── precision & speed ──────────────────────────────────────────────────────
  fp16: true                             # enable mixed precision
  bf16: false                            # turn off to avoid dual precision modes
  # torch_dtype: "auto"                 # (optional) lets HF pick fastest dtype

  # ── optimiser & scheduler ─────────────────────────────────────────────────
  learning_rate: 0.00005                    # good starting LR for GPT‑2 on small corpora
  warmup_ratio: 0.1

  # ── misc performance knobs ────────────────────────────────────────────────
  dataloader_num_workers: 4
  gradient_checkpointing: true           # big memory win on GPT‑style decoders
  max_grad_norm: 1.0

  # ── logging, saving, early stop ───────────────────────────────────────────
  logging_steps: 100
  eval_steps: 500
  save_steps: 500
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false

  use_early_stopping: true
  early_stopping_patience: 2
  early_stopping_threshold: 0.0005

  do_test_eval: true
